{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test RecSys for suggesting answers to customer questions, in order to help senior customer service executives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU\n",
    "from numpy.random import randint, uniform\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from keras import backend as K\n",
    "from icecream import ic\n",
    "from typing import Union\n",
    "import jellyfish as jello\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business users explicitly asked to provide a list of topics that a client can ask about and the basic set of acceptable answers\n",
    "topics = {\n",
    "    'topic1' : ['bla', 'blabla', 'blablabla'],\n",
    "    'some question': ['generic answer', 'specific answer', 'customized answer']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_parquet(\"recommender_inputs/real_output.gzip\") # one pair of question and answer with metadata per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_found = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_temas(seq: str) -> str:\n",
    "    global topics, topics_found\n",
    "    '''\n",
    "    Assign a predetermined topic to each Q&A pair\n",
    "    '''\n",
    "    if seq in topics_found.keys():\n",
    "        return topics_found[seq]\n",
    "    else:\n",
    "        words = seq.split(\" \")\n",
    "        temas = {}\n",
    "        tt = 0\n",
    "        for w in words:\n",
    "            for k in topics.keys():\n",
    "                research = True\n",
    "                for t in topics[k]:\n",
    "                    if k != \"default\":\n",
    "                        if int(len(w)* 0.1) >= jello.damerau_levenshtein_distance(w.lower(), t) or t in w.lower():\n",
    "                            if k not in temas.keys():\n",
    "                                temas[k] = 1\n",
    "                            else:\n",
    "                                temas[k] = temas[k] + 1\n",
    "                            tt += 1\n",
    "                            research = False\n",
    "                            break\n",
    "                if tt == 3 or not research:\n",
    "                    break\n",
    "            if tt == 3:\n",
    "                break\n",
    "        if len(list(temas.keys())) == 0:\n",
    "            topics_found[seq] = \"default\"\n",
    "            return \"default\"\n",
    "        else:\n",
    "            final = list(dict(sorted(temas.items(), key=lambda x: x[1], reverse=True)).keys())\n",
    "            topics_found[seq] = final[0]\n",
    "            return final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.loc[:, \"Tema\"] = input_df.loc[:, \"cliente\"].apply(buscar_temas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(output_df_atom:pd.DataFrame, embedding_size=250, vocab_size=10000):\n",
    "    \"\"\" Generar matrices con los embeddings de las dimensiones especificadas\n",
    "\n",
    "    Args:\n",
    "        output_df_atom (pd.DataFrame): Dataframe de pares mensaje cliente - acción tomada a codificar\n",
    "        embedding_size (int, optional): Tamaño del vector de embedding de cada mensaje. Defaults to 250.\n",
    "        vocab_size (int, optional): Tamaño del vocabulario a procesar como máximo. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    tokenizer = text.Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(output_df_atom[\"accion_ejecutivo\"] + output_df_atom[\"mensaje_cliente\"].apply(lambda x: \" \" + x))\n",
    "\n",
    "    def prep_text(texts, tokenizer, max_sequence_length):\n",
    "        # Turns text into into padded sequences.\n",
    "        text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "        return sequence.pad_sequences(text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    text_input = prep_text(output_df_atom[\"mensaje_cliente\"], tokenizer, embedding_size)\n",
    "    text_action = prep_text(output_df_atom[\"accion_ejecutivo\"], tokenizer, embedding_size)\n",
    "\n",
    "    return text_input, text_action, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_text(list_of_indices):\n",
    "    # Function takes a tokenized sentence and returns the words\n",
    "    # Looking up words in dictionary\n",
    "    # Creating a reverse dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    words = [w for w in words if not w is None]\n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(id_msje:int) -> str:\n",
    "    \"\"\" Traducir ids de mensajes a los mensajes en texto plano\n",
    "\n",
    "    Args:\n",
    "        id_msje (int): Id representativo de embeddings\n",
    "\n",
    "    Returns:\n",
    "        str: String del mensaje correspondiente al embedding\n",
    "    \"\"\"\n",
    "    global originals\n",
    "    seq = originals[id_msje].astype(np.int32).tolist()\n",
    "    # Creating texts\n",
    "    my_texts = list(map(sequence_to_text, [seq]))\n",
    "    return \" \".join(my_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_LEN = 250       # number N of points where a curve is sampled\n",
    "SAMPLE_SIZE = len(input_df)   # number of curves in the training set\n",
    "EPOCHS = 100\n",
    "BATCH = 100 # number of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de entrada / Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estas siguientes celdas son mandatorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy(input_df: pd.DataFrame) -> Union[pd.DataFrame, text.Tokenizer, dict]:\n",
    "    '''\n",
    "    Create a dummy vector per original row to represent a phrase as an integer of less than 16 bits.\n",
    "    '''\n",
    "    input_df = input_df.copy().reset_index() # ensure creation of local variable\n",
    "    text_input, text_action, tokenizer = generate_embeddings(input_df)\n",
    "    metadata = input_df[[\"weekday\", \"day\", \"month\"]].to_numpy()\n",
    "    input_data = np.concatenate((text_input, metadata), axis=1)\n",
    "\n",
    "    # Crear pares item - \"usuario\", donde el usuario es el mensaje del usuario y el item es el mensaje del ejecutivo. Es para usar cualquier recomendador común.\n",
    "    dummy = pd.DataFrame(columns=[\"UserId\", \"ActionId\", \"Rating\", \"Timestamp\"], index = range(input_data.shape[0]))\n",
    "    originals = {}\n",
    "\n",
    "    for index in range(input_data.shape[0]):\n",
    "        iden = input_data[index].tolist()\n",
    "        identificador = ''\n",
    "        for num in iden:\n",
    "            identificador += str(int(num))\n",
    "        identificador = identificador[len(identificador)-13:len(identificador)-4]\n",
    "        input_id = int(identificador)\n",
    "        dummy.at[index, \"UserId\"] = input_id\n",
    "        if input_id not in originals.keys():\n",
    "            originals[input_id] = text_input[index]\n",
    "\n",
    "    for index in range(text_action.shape[0]):\n",
    "        iden = text_action[index].tolist()\n",
    "        identificador = ''\n",
    "        for num in iden:\n",
    "            identificador += str(int(num))\n",
    "        identificador = identificador[len(identificador)-13:len(identificador)-4]\n",
    "        input_id = int(identificador)\n",
    "        dummy.at[index, \"ActionId\"] = input_id\n",
    "        if input_id not in originals.keys():\n",
    "            originals[input_id] = text_action[index]\n",
    "\n",
    "    dummy[\"Rating\"] = input_df[\"Puntaje encuesta\"]\n",
    "    dummy[\"Timestamp\"] = input_df[\"Timestamp\"].astype(np.int32)\n",
    "    return dummy, tokenizer, originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = create_dummy(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación recsys basado en contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.utils.notebook_utils import is_jupyter\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 50\n",
    "\n",
    "# Column names for the dataset\n",
    "COL_USER = \"UserId\"\n",
    "COL_ITEM = \"ActionId\"\n",
    "COL_RATING = \"Rating\"\n",
    "COL_TIMESTAMP = \"Timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\")\n",
    "# spark = SparkSession.builder.master(\"spark://192.168.68.63:8080\").getOrCreate()\n",
    "# spark.conf.set(\"spark.executor.memory\", \"16g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    (\n",
    "        StructField(COL_USER, IntegerType()),\n",
    "        StructField(COL_ITEM, IntegerType()),\n",
    "        StructField(COL_RATING, FloatType()),\n",
    "        StructField(COL_TIMESTAMP, LongType()),\n",
    "    )\n",
    ")\n",
    "# enableing the Apache Arrow for converting\n",
    "# Pandas to pySpark DF(DataFrame)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# Creating the DataFrame\n",
    "data = spark.createDataFrame(dummy, schema=schema) # .dropDuplicates(['UserId'])\n",
    "# data = movielens.load_spark_df(spark, size=MOVIELENS_DATA_SIZE, schema=schema)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = spark_random_split(data, ratio=0.75, seed=123)\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"userCol\": COL_USER,\n",
    "    \"itemCol\": COL_ITEM,\n",
    "    \"ratingCol\": COL_RATING,\n",
    "}\n",
    "\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=42,\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as train_time:\n",
    "    model = als.fit(train)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as test_time:\n",
    "\n",
    "    # Get the cross join of all user-item pairs and score them.\n",
    "    users = train.select(COL_USER).distinct()\n",
    "    items = train.select(COL_ITEM).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    dfs_pred = model.transform(user_item).alias(\"pred\").withColumnRenamed('UserId', 'User_id').withColumnRenamed('ActionId', 'Action_id')\n",
    "    train = train.alias(\"train\")\n",
    "\n",
    "    # ic(users.count(), items.count(), dfs_pred.count())\n",
    "\n",
    "    # Remove seen items.\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train.alias(\"train\"),\n",
    "        [dfs_pred.User_id.alias(\"User_id\") == train.UserId, dfs_pred.Action_id.alias(\"Action_id\") == train.ActionId],\n",
    "        how='outer'\n",
    "    ).select(\"User_id\", \"Rating\", \"Action_id\", \"prediction\")# .withColumnRenamed(train.ActionId, 'Action_Id')\n",
    "\n",
    "    dfs_pred_exclude_train = dfs_pred_exclude_train.withColumnRenamed('User_id', COL_USER).withColumnRenamed('Action_id', COL_ITEM).alias(\"pred\")\n",
    "\n",
    "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"{COL_RATING}\"].isNull()) \\\n",
    "        .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
    "\n",
    "    # In Spark, transformations are lazy evaluation\n",
    "    # Use an action to force execute and measure the test time\n",
    "    top_all.cache().count()\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_all.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM,\n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\",\n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model:\\tALS\",\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predicted ratings.\n",
    "prediction = model.transform(test)\n",
    "prediction.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_predictions = prediction.toPandas()\n",
    "pd_predictions.loc[:, \"ActionId\"] = pd_predictions[\"ActionId\"].apply(translate)\n",
    "pd_predictions.loc[:, \"UserId\"] = pd_predictions[\"UserId\"].apply(translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
    "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
    "\n",
    "print(\"Model:\\tALS rating prediction\",\n",
    "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
    "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
    "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
    "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación Bi VAE (adapted from docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import cornac\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from recommenders.models.cornac.cornac_utils import predict_ranking\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "print(\"Cornac version: {}\".format(cornac.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# top k items to recommend\n",
    "TOP_K = 50\n",
    "\n",
    "# Model parameters\n",
    "LATENT_DIM = 50\n",
    "ENCODER_DIMS = [100]\n",
    "ACT_FUNC = \"tanh\"\n",
    "LIKELIHOOD = \"pois\"\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiVAE_suite(input_df:pd.DataFrame, save: bool, predict: bool) -> dict:\n",
    "    \"\"\" Entrena diversas BiVAE por tema de conversación\n",
    "\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): Datos preprocesados\n",
    "        save (bool): True si se quieren guardar datos localmente (NO USAR EN COLAB)\n",
    "        predict (bool): True si se quiere hacer predicción con la BiVAE recién entrenada (usa mucha memoria, cuidado!)\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario donde los keys son los temas y los valores son: (BiVAE, tokenizer utiliado para codificar frases, diccionario que traduce simplificaciones de frases a frases completas)\n",
    "    \"\"\"\n",
    "    global topics, TOP_K, LATENT_DIM, ENCODER_DIMS, ACT_FUNC, LIKELIHOOD, NUM_EPOCHS, BATCH_SIZE, LEARNING_RATE\n",
    "    temas = list(topics.keys())\n",
    "    suite = {}\n",
    "    for tema in temas:\n",
    "        print(\"---------------------- Topic to train: {} ----------------------\".format(tema))\n",
    "        dummy, tokenizer, originals = create_dummy(input_df, tema)\n",
    "        if len(dummy) >= 4:\n",
    "            train, test = python_random_split(dummy, 0.75)\n",
    "            test.rename(columns = {'UserId':'userID'}, inplace = True)\n",
    "            test.rename(columns = {'Rating':'rating'}, inplace = True)\n",
    "            test.rename(columns = {'ActionId':'itemID'}, inplace = True)\n",
    "\n",
    "            train.rename(columns = {'UserId':'userID'}, inplace = True)\n",
    "            train.rename(columns = {'Rating':'rating'}, inplace = True)\n",
    "            train.rename(columns = {'ActionId':'itemID'}, inplace = True)\n",
    "            train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)\n",
    "\n",
    "            print('Number of users: {}'.format(train_set.num_users))\n",
    "            print('Number of items: {}'.format(train_set.num_items))\n",
    "            bivae = cornac.models.BiVAECF(\n",
    "                k=LATENT_DIM,\n",
    "                encoder_structure=ENCODER_DIMS,\n",
    "                act_fn=ACT_FUNC,\n",
    "                likelihood=LIKELIHOOD,\n",
    "                n_epochs=NUM_EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED,\n",
    "                use_gpu=torch.cuda.is_available(),\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            with Timer() as t:\n",
    "                bivae.fit(train_set)\n",
    "            print(\"Topic {}: Took {} seconds for training.\".format(tema, t))\n",
    "            if save:\n",
    "                file = open('BiVAE_{}.pkl'.format(tema), 'wb')\n",
    "                pickle.dump(bivae, file)\n",
    "                file.close()\n",
    "                file = open('tokenizer_{}.pkl'.format(tema), 'wb')\n",
    "                pickle.dump(tokenizer, file)\n",
    "                file.close()\n",
    "                file = open('originals_{}.pkl'.format(tema), 'wb')\n",
    "                pickle.dump(originals, file)\n",
    "                file.close()\n",
    "            suite[tema] = (bivae, tokenizer, originals)\n",
    "            if predict:\n",
    "                with Timer() as t:\n",
    "                    all_predictions = predict_ranking(bivae, train, usercol='userID', itemcol='itemID', remove_seen=True)\n",
    "                print(\"Took {} seconds for prediction.\".format(t))\n",
    "                all_predictions.loc[:,\"prediction\"] = all_predictions.prediction.astype(np.int32)\n",
    "                test.loc[:,\"rating\"] = test[\"rating\"].astype(np.int32)\n",
    "                all_predictions.rename(columns = {'ActionId':'itemID'}, inplace = True)\n",
    "                all_predictions.rename(columns = {'UserId':'userID'}, inplace = True)\n",
    "                test.loc[:,\"rating\"] = test[\"rating\"].astype(np.int32)\n",
    "                eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "                eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "                eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "                eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "\n",
    "                print(\"MAP:\\t%f\" % eval_map,\n",
    "                        \"NDCG:\\t%f\" % eval_ndcg,\n",
    "                        \"Precision@K:\\t%f\" % eval_precision,\n",
    "                        \"Recall@K:\\t%f\" % eval_recall, sep='\\n')\n",
    "        else:\n",
    "            print(\"Topic {} has less than 4 message pairs...\".format(tema))\n",
    "    return suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiVAE_suite(input_df, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación GAN (basic proof of concept for automatic phrase suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versión más básica (usar esta por mientras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = Dropout(0.4)        # Empirical hyperparameter\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(SAMPLE_LEN, activation=\"relu\"))\n",
    "discriminator.add(DROPOUT)\n",
    "discriminator.add(Dense(SAMPLE_LEN, activation=\"relu\"))\n",
    "discriminator.add(DROPOUT)\n",
    "discriminator.add(Dense(SAMPLE_LEN, activation=\"relu\"))\n",
    "discriminator.add(DROPOUT)\n",
    "discriminator.add(Dense(SAMPLE_LEN, activation=\"relu\"))\n",
    "discriminator.add(DROPOUT)\n",
    "discriminator.add(Dense(SAMPLE_LEN, activation=\"relu\"))\n",
    "discriminator.add(DROPOUT)\n",
    "discriminator.add(Dense(1, activation = \"sigmoid\"))\n",
    "discriminator.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador (choose one version and execute only that cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1 (más simple, peor en entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAKY_RELU = LeakyReLU(0.2)   # Empirical hyperparameter\n",
    "generator = Sequential()\n",
    "generator.add(tf.keras.layers.Embedding(2*SAMPLE_LEN**2, SAMPLE_LEN*2, input_length = in_len))\n",
    "generator.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "generator.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(SAMPLE_LEN, dropout=0.05, recurrent_dropout=0.2)))\n",
    "generator.add(tf.keras.layers.Activation('softmax'))\n",
    "#generator.add(Dense(SAMPLE_LEN))\n",
    "#generator.add(LEAKY_RELU)\n",
    "generator.add(Dense(512))\n",
    "generator.add(LEAKY_RELU)\n",
    "generator.add(Dense(SAMPLE_LEN, activation = \"tanh\"))\n",
    "generator.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2 (más compleja, captura más datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    We redefine our own loss function in order to get rid of the '0' value\n",
    "    which is the one used for padding. This to avoid that the model optimize itself\n",
    "    by predicting this value because it is the padding one.\n",
    "    \n",
    "    :param real: the truth\n",
    "    :param pred: predictions\n",
    "    :return: a masked loss where '0' in real (due to padding)\n",
    "                are not taken into account for the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # to check that pred is numric and not nan\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                 reduction='none')\n",
    "    loss_ = loss_object_(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAKY_RELU = LeakyReLU(0.2)   # Empirical hyperparameter\n",
    "_input = tf.keras.layers.Input(shape=[in_len], dtype='int32')\n",
    "generate = tf.keras.layers.Embedding(2*SAMPLE_LEN**2, SAMPLE_LEN*2, input_length = in_len, mask_zero=True)(_input)\n",
    "generate = tf.keras.layers.SpatialDropout1D(0.2)(generate)\n",
    "generate = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(SAMPLE_LEN, dropout=0.05, recurrent_dropout=0.2, return_sequences=True))(generate)\n",
    "generate = tf.keras.layers.SpatialDropout1D(0.2)(generate)\n",
    "generate = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(SAMPLE_LEN, dropout=0.05, recurrent_dropout=0.2, return_sequences=True))(generate)\n",
    "generate = tf.keras.layers.SpatialDropout1D(0.2)(generate)\n",
    "generate = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(SAMPLE_LEN, dropout=0.05, recurrent_dropout=0.2, return_sequences=True))(generate)\n",
    "\n",
    "# create encoding padding mask\n",
    "encoding_padding_mask = tf.math.logical_not(tf.math.equal(_input, 0))\n",
    "\n",
    "# Self attention so key=value in inputs\n",
    "att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
    "                                name='attention')(inputs=[generate, generate],\n",
    "                                                    mask=[encoding_padding_mask,\n",
    "                                                        encoding_padding_mask])\n",
    "# generator_tail = tf.keras.layers.TimeDistributed(Dense(SAMPLE_LEN, activation='sigmoid'))(att)\n",
    "#generator.add(Dense(SAMPLE_LEN))\n",
    "#generator.add(LEAKY_RELU)\n",
    "query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    generate)\n",
    "att = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    att)\n",
    "RNN_attention_parallel = tf.keras.layers.Concatenate()(\n",
    "    [query_encoding, att])\n",
    "generator_tail = Dense(SAMPLE_LEN, activation = \"tanh\")(RNN_attention_parallel)\n",
    "#generator.add(LEAKY_RELU)\n",
    "\n",
    "generator = tf.keras.Model(inputs=_input, outputs=generator_tail)\n",
    "generator.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss=\"categorical_crossentropy\", metrics = ['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar / Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = Sequential()\n",
    "gan.add(generator)\n",
    "gan.add(discriminator)\n",
    "gan.compile(optimizer = \"adam\", loss=\"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONES = np.ones((SAMPLE_SIZE))\n",
    "ZEROS = np.zeros((SAMPLE_SIZE))\n",
    "\n",
    "print(\"epoch | dis. loss | dis. acc | gen. loss | gen. acc\")\n",
    "print(\"------+-----------+----------+-----------+----------\")\n",
    "\n",
    "for e in tqdm(range(EPOCHS)):\n",
    "    for k in range(SAMPLE_SIZE//BATCH):\n",
    "        # Addestra il discriminatore a riconoscere le sinusoidi vere da quelle prodotte dal generatore\n",
    "        n = randint(0, in_len, size = BATCH)\n",
    "        # Ora prepara un batch di training record per il discriminatore\n",
    "        # print(input_data[n].shape)\n",
    "        p = generator.predict(input_data[n])\n",
    "        # print(text_action[n].shape, p.shape)\n",
    "        x = np.concatenate((text_action[n], p))\n",
    "        y = np.concatenate((ONES[n], ZEROS[n]))\n",
    "        d_result = discriminator.train_on_batch(x, y)\n",
    "        discriminator.trainable = False\n",
    "        g_result = gan.train_on_batch(input_data[n], ONES[n])\n",
    "        discriminator.trainable = True\n",
    "    print(f\" {e:04n} |  {d_result[0]:.5f}  |  {d_result[1]:.5f} |  {g_result[0]:.5f}  |  {d_result[1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87f5017a5dc955675d756efa69030300a49a19bb3ddb510d2d48fdcda101afff"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
