{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple summarization of paper abstracts demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read papers in PDF and collect abstract from the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "top = os.getcwd()\n",
    "papers = []\n",
    "\n",
    "for root, dirs, files in os.walk(top, topdown=False):\n",
    "    for fl in files:\n",
    "        if \"paper_\" in fl:\n",
    "            with fitz.open(fl) as doc:\n",
    "                text = \"\"\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "                    break\n",
    "                papers.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "dividers = [\"Abstract\", \"abstract\", \"ABSTRACT\"]\n",
    "finishers = [\"INTRODUCTION\", \"Introduction\", \"introduction\"]\n",
    "for paper in papers:\n",
    "    abstract = \" \".join(paper.split(\"\\n\"))\n",
    "    for div in dividers:\n",
    "        if div in abstract:\n",
    "            abstract = abstract.split(div)[1]\n",
    "            break\n",
    "    for fin in finishers:\n",
    "        abstract = abstract.split(fin)[0]\n",
    "    abstract = abstract.split(\"doi:\")[0].split(\"DOI:\")[0].strip(\"/\").strip(\":\").strip(\"https\")\n",
    "    abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize using Pegasus transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "summaries = []\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "for src_text in abstracts:\n",
    "    batch = tokenizer([src_text], truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "    translated = model.generate(**batch)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    summaries.append(tgt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging section: Print abstracts and their summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "All photographs are copyrighted.\n",
      " ----------- Abstract:\n",
      " ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2019 Association for Computing Machinery. XXXX-XXXX/2019/4-ART $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn , Vol. 1, No. 1, Article . Publication date: April 2019. arXiv:1901.06796v3  [cs.CL]  11 Apr 2019 \n",
      "------------------------\n",
      "Summary:\n",
      "The theme of this year's World Health Organization (WHO) World Health Day is autism spectrum disorder.\n",
      " ----------- Abstract:\n",
      "  Autism spectrum disorder is a term used to describe a constellation of early-appearing social  communication deficits and repetitive sensory–motor behaviours associated with a strong genetic  component as well as other causes. The outlook for many individuals with autism spectrum  disorder today is brighter than it was 50 years ago; more people with the condition are able to  speak, read, and live in the community rather than in institutions, and some will be largely free  from symptoms of the disorder by adulthood. Nevertheless, most individuals will not work full- time or live independently. Genetics and neuroscience have identified intriguing patterns of risk,  but without much practical benefit yet. Considerable work is still needed to understand how and  when behavioural and medical treatments can be effective, and for which children, including those  with substantial comorbidities. It is also important to implement what we already know and  develop services for adults with autism spectrum disorder. Clinicians can make a difference by  providing timely and individualised help to families navigating referrals and access to community  support systems, by providing accurate information despite often unfiltered media input, and by  anticipating transitions such as family changes and school entry and leaving. \n",
      "------------------------\n",
      "Summary:\n",
      "This article is copyrighted.\n",
      " ----------- Abstract:\n",
      "  Although there are a large and growing number of scientiﬁcally ques- tionable treatments available for children with autism spectrum disorder (ASD), intervention programs applying the scientiﬁc teaching principles of applied behavior analysis (ABA) have been identiﬁed as the treatment of choice. The following article provides a selective review of ABA in- tervention approaches, some of which are designed as comprehensive programs that aim to address all developmental areas of need, whereas others are skills based or directed toward a more circumscribed, speciﬁc set of goals. However, both types of approaches have been shown to be effective in improving communication, social skills, and management of problem behavior for children with ASD. Implications of these ﬁndings are discussed in relation to critical areas of research that have yet to be fully explored. 447 Annu. Rev. Clin. Psychol. 2010.6:447-468. Downloaded from www.annualreviews.org by Georgetown University Medical Center- DAHLGREN MEDICAL LIBRARY on 03/29/11. For personal use only. Click here for quick links to  Annual Reviews content online,  including: ��Other articles in this volume � Top cited articles � Top downloaded articles ����������rehensive search Further ANNUAL REVIEWS Click here for quick links to  Annual Reviews content online,  including: ��Other articles in this volume � Top cited articles � Top downloaded articles ����������rehensive search Further ANNUAL REVIEWS \n",
      "------------------------\n",
      "Summary:\n",
      "Natural language processing (NLP) is one of the fastest growing areas of computer science.\n",
      " ----------- Abstract:\n",
      "  Pre-trained text encoders have rapidly ad- vanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic informa- tion is captured within the network. We ﬁnd that the model represents the steps of the tra- ditional NLP pipeline in an interpretable and localizable way, and that the regions respon- sible for each step appear in the expected se- quence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does ad- just this pipeline dynamically, revising lower- level decisions on the basis of disambiguating information from higher-level representations. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "We propose a new approach for embedding Transformer models with positional information.\n",
      " ----------- Abstract:\n",
      "  Without positional information, attention-based Transformer neural networks are permutation-invariant. Absolute or relative positional embeddings are the most popular ways to feed Transformer models with positional information. Absolute positional embeddings are simple to implement, but suffer from generalization issues when evaluating on sequences longer than seen at training time. Relative po- sitions are more robust to input length change, but are more complex to implement and yield inferior model throughput due to extra computational and memory costs. In this paper, we propose an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute (simplicity and speed) and relative positional embeddings (better generalization). In addition, our empirical evaluation on state-of-the-art models in machine translation, im- age and speech recognition demonstrates that CAPE leads to better generalization performance as well as increased stability with respect to training hyper-parameters. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "A selection of photos from around the world this week:\n",
      " ----------- Abstract:\n",
      " \n",
      "------------------------\n",
      "Summary:\n",
      "The performance of word-vec- tors on sentence-level classification tasks has been improved by a simple tuning of the ar- chitecture.\n",
      " ----------- Abstract:\n",
      "  We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vec- tors for sentence-level classiﬁcation tasks. We show that a simple CNN with lit- tle hyperparameter tuning and static vec- tors achieves excellent results on multi- ple benchmarks. Learning task-speciﬁc vectors through ﬁne-tuning offers further gains in performance. We additionally propose a simple modiﬁcation to the ar- chitecture to allow for the use of both task-speciﬁc and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classiﬁcation. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "Researchers at the Massachusetts Institute of Technology (MIT) have developed a new type of machine learning.\n",
      " ----------- Abstract:\n",
      " Deep Learning for  Natural Language  Processing Creating Neural Networks   with Python Palash Goyal Sumit Pandey Karan Jain \n",
      "------------------------\n",
      "Summary:\n",
      "A computer system is developing the model for speech synthesis of various aspects for natural language processing.\n",
      " ----------- Abstract:\n",
      "  The computer system is developing the model for speech synthesis of various aspects for natural language processing.  The speech synthesis explores by articulatory, formant and concatenate synthesis. These techniques lead more aperiodic distortion  and give exponentially increasing error rate during process of the system. Recently, advances on speech synthesis are tremendously  moves towards deep learning process in order to achieve better performance. Due to leverage of large scale data gives effective  feature representations to speech synthesis. The main objective of this research article is that implements deep learning techniques  into speech synthesis and compares the performance in terms of aperiodic distortion with prior model of algorithms in natural  language processing.    Keywords: Natural Language Processing, speech synthesis, deep learning     1. \n",
      "------------------------\n",
      "Summary:\n",
      "Graph-structured data such as sequences, trees, and graph data (e.g.\n",
      " ----------- Abstract:\n",
      "  meaning rep- resentation (AMR) graphs) can be exploited to aug- ment original sequence data by incorporating the task-speciﬁc knowledge. As a result, these graph- structured data can encode complicated pairwise relationships between entity tokens for learning more informative representations. However, it is well-known that deep learning techniques that were disruptive for Euclidean data such as images or se- quence data such as text are not immediately appli- cable to graph-structured data. Therefore, this gap has driven a tide in research for deep learning on graphs, especially in development of graph neural networks (GNN). This wave of research at the intersection of deep learning on graphs and NLP has inﬂuenced a vari- ety of NLP tasks. There has seen a surge of inter- ests in applying/developing various types of GNNs and achieved considerable success in many NLP tasks, ranging from classiﬁcation tasks like sen- tence classiﬁcation, semantic role labeling and re- lation extraction, to generation tasks like machine translation, question generation and summarization. Despite these successes, deep learning on graphs for NLP still face many challenges, namely, • Automatically transforming original text se- quence data into highly graph-structured data. Such challenges are profound in NLP since most of the NLP tasks use the text sequence as the original inputs. Automatic graph con- struction from the text sequence to take into account underlying structural information is a critical step in the use of graph neural network models for NLP problems. • Effectively modeling complex data that in- volves mapping between graph-based inputs and other highly structured output data such as sequences, trees, and graph data with multi- types in both nodes and edges. Many genera- tion tasks in NLP such as SQL-to-Text, Text- to-AMR, Text-to-KB are emblematic of such challenges. This tutorial of Deep Learning on Graphs for Natural Language Processing (DLG4NLP) is timely for the computational linguistics community, and covers relevant and interesting topics, includ- ing automatic graph construction for NLP, graph representation learning for NLP, various advanced GNN based models (e.g., graph2seq, graph2tree, \n",
      "------------------------\n",
      "Summary:\n",
      "The aim of this paper is to provide an overview of generative adversarial networks (GANs).\n",
      " ----------- Abstract:\n",
      "  Generative adversarial networks are a kind of artificial intel- ligence algorithm designed to solve the generative model- ing problem. The goal of a generative model is to study a  collection of training examples and learn the probability  distribution that generated them. Generative Adversarial  Networks (GANs) are then able to generate more examples  from the estimated probability distribution. Generative  models based on deep learning are common, but GANs  are among the most successful generative models (espe- cially in terms of their ability to generate realistic high- resolution images). GANs have been successfully applied  to a wide variety of tasks (mostly in research settings) but  continue to present unique challenges and research  opportunities because they are based on game theory  while most other approaches to generative modeling are  based on optimization. 1. \n",
      "------------------------\n",
      "Summary:\n",
      "In this paper, we present a novel method for learning word vectors from large data sets.\n",
      " ----------- Abstract:\n",
      "  We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "We present a new approach to the problem of adversarial training.\n",
      " ----------- Abstract:\n",
      "  Several machine learning models, including neural networks, consistently mis- classify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed in- put results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to ad- versarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Us- ing this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "Fairness is an important issue in the financial services industry.\n",
      " ----------- Abstract:\n",
      "  We present a machine learning pipeline for fairness-aware machine learning (FAML) in ﬁ- nance that encompasses metrics for fairness (and accuracy). Whereas accuracy metrics are well un- derstood and the principal ones used frequently, there is no consensus as to which of several avail- able measures for fairness should be used in a generic manner in the ﬁnancial services indus- try. We explore these measures and discuss which ones to focus on, at various stages in the ML pipeline, pre-training and post-training, and we also examine simple bias mitigation approaches. Using a standard dataset we show that the se- quencing in our FAML pipeline offers a cogent approach to arriving at a fair and accurate ML model. We discuss the intersection of bias met- rics with legal considerations in the US, and the entanglement of explainability and fairness is ex- empliﬁed in the case study. We discuss possible approaches for training ML models while satis- fying constraints imposed from various fairness metrics, and the role of causality in assessing fair- ness. 1. \n",
      "------------------------\n",
      "Summary:\n",
      "In this paper we present a new architecture for deep neural networks.\n",
      " ----------- Abstract:\n",
      "  We introduce the “exponential linear unit” (ELU) which speeds up learning in deep neural networks and leads to higher classiﬁcation accuracies. Like recti- ﬁed linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe- LUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gra- dient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of pres- ence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to signiﬁcantly bet- ter generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks signiﬁcantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classiﬁcation error for a single crop, single model network. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "In this paper, we present a new approach to the problem of designing mod- els for classification problems.\n",
      " ----------- Abstract:\n",
      "  We consider the problem of designing mod- els to leverage a recently introduced ap- proximate model averaging technique called dropout. We deﬁne a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a nat- ural companion to dropout) designed to both facilitate optimization by dropout and im- prove the accuracy of dropout’s fast approxi- mate model averaging technique. We empir- ically verify that the model successfully ac- complishes both of these tasks. We use max- out and dropout to demonstrate state of the art classiﬁcation performance on four bench- mark datasets: MNIST, CIFAR-10, CIFAR- 100, and SVHN. 1. \n",
      "------------------------\n",
      "Summary:\n",
      "In this paper, we present a novel approach for self-supervised speech representation.\n",
      " ----------- Abstract:\n",
      " —Self-supervised approaches for speech representa- tion learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an ofﬂine clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of- the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h ﬁne-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.1 Index Terms—Self-supervised learning, BERT. I. \n",
      "------------------------\n",
      "Summary:\n",
      "In this paper, we present the first large-scale comparison of 21 NLP activation functions across eight different NLP tasks.\n",
      " ----------- Abstract:\n",
      "  Activation functions play a crucial role in neural networks because they are the non- linearities which have been attributed to the success story of deep learning. One of the currently most popular activation functions is ReLU, but several competitors have re- cently been proposed or ‘discovered’, includ- ing LReLU functions and swish. While most works compare newly proposed activa- tion functions on few tasks (usually from im- age classiﬁcation) and against few competitors (usually ReLU), we perform the ﬁrst large- scale comparison of 21 activation functions across eight different NLP tasks. We ﬁnd that a largely unknown activation function performs most stably across all tasks, the so-called pe- nalized tanh function. We also show that it can successfully replace the sigmoid and tanh gates in LSTM cells, leading to a 2 percent- age point (pp) improvement over the standard choices on a challenging NLP task. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "All photographs are copyrighted.\n",
      " ----------- Abstract:\n",
      "                                               King’s Research Portal    \n",
      "------------------------\n",
      "Summary:\n",
      "Virtual assistants are increasingly being used to fill slots in online games.\n",
      " ----------- Abstract:\n",
      "  We present the MASSIVE dataset— Multilingual Amazon Slu resource package (SLURP) for Slot-ﬁlling, Intent classiﬁcation, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classiﬁcation accuracy, and slot-ﬁlling F1 score. We have released our dataset, modeling code, and models publicly. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "Several extensions to the Skip-gram model for learning vector representations have been proposed.\n",
      " ----------- Abstract:\n",
      "  The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for ﬁnding phrases in text, and show that learning good vector representations for millions of phrases is possible. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "We propose a new approach to the training of machine translation models.\n",
      " ----------- Abstract:\n",
      "  The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. 1 \n",
      "------------------------\n",
      "Summary:\n",
      "In this paper, we present a novel architecture for training vision transformers.\n",
      " ----------- Abstract:\n",
      "  Transformers yield state-of-the-art results across many tasks. However, they im- pose huge computational costs during inference. We apply global structural pruning with latency-aware regularization on all parameters of the Vision Transformer (ViT) model for latency reduction. Furthermore, we analyze the pruned architectures and ﬁnd interesting regularities in the ﬁnal weight structure. Our discovered insights lead to a new architecture called NViT (Novel ViT), with a redistribution of where parameters are used. This architecture utilizes parameters more efﬁciently and enables control of the latency-accuracy trade-off. On ImageNet-1K, we prune the DEIT-Base (Touvron et al., 2021) model to a 2.6× FLOPs reduction, 5.1× param- eter reduction, and 1.9× run-time speedup with merely 0.07% loss in accuracy. We achieve more than 1% accuracy gain when compressing the base model to the throughput of the Small/Tiny variants. NViT gains 0.1-1.1% accuracy over the hand-designed DEIT family when trained from scratch, while being faster. 1 \n",
      "------------------------\n",
      "Summary:\n",
      ".\n",
      " ----------- Abstract:\n",
      " ive Summarization Jingqing Zhang * 1 Yao Zhao * 2 Mohammad Saleh 2 Peter J. Liu 2 \n",
      "------------------------\n",
      "Summary:\n",
      "Population cancer registries can benefit from Deep Learning to automatically extract cancer characteristics from the high volume of unstructured pathology text reports they process annually. Population cancer registries can benefit from Deep Learning (DL) to automatically extract cancer characteristics from the high volume of unstructured pathology text reports they process annually.\n",
      " ----------- Abstract:\n",
      "  Population cancer registries can beneﬁt from Deep Learning (DL) to automatically extract cancer characteristics from the high volume of unstructured pathology text reports they process annually. The success of DL to tackle this and other real-world problems is proportional to the availability of large labeled datasets for model training. Although collaboration among cancer registries is essential to fully exploit the promise of DL, privacy and conﬁdentiality concerns are main obstacles for data sharing across cancer registries. Moreover, DL for natural language processing (NLP) requires sharing a vocabulary dictionary for the embedding layer which may contain patient identiﬁers. Thus, even distributing the trained models across cancer registries causes a pri- vacy violation issue. In this article, we propose DL NLP model distribution via privacy-preserving transfer learn- ing approaches without sharing sensitive data. These approaches are used to distribute a multitask convolutional neural network (MT-CNN) NLP model among cancer registries. The model is trained to extract six key cancer characteristics – tumor site, subsite, laterality, behavior, histology, and grade – from cancer pathology reports. Using 410,064 pathology documents from two cancer registries, we compare our proposed approach to conven- tional transfer learning without privacy-preserving, single-registry models, and a model trained on centrally hosted data. The results show that transfer learning approaches including data sharing and model distribution out- perform signiﬁcantly the single-registry model. In addition, the best performing privacy-preserving model distri- bution approach achieves statistically indistinguishable average micro- and macro-F1 scores across all extraction tasks (0.823,0.580) as compared to the centralized model (0.827,0.585). INDEX TERMS Privacy-preserving, multi-task CNN, transfer learning, NLP, information extraction, cancer pathology reports I. \n",
      "------------------------\n",
      "Summary:\n",
      "The relationship between the quantum state of a matter source and its graviton field is investigated.\n",
      " ----------- Abstract:\n",
      " Quantum Hair from Gravity Xavier Calmet ,1,* Roberto Casadio ,2,3,† Stephen D. H. Hsu,4,‡ and Folkert Kuipers 1,§ 1Department of Physics and Astronomy, University of Sussex, Brighton BN1 9QH, United Kingdom 2Dipartimento di Fisica e Astronomia, Universit`a di Bologna, via Irnerio 46, I-40126 Bologna, Italy 3I.N.F.N., Sezione di Bologna, IS—FLAG, via B. Pichat 6/2, I-40127 Bologna, Italy 4Department of Physics and Astronomy Michigan State University, East Lansing, Michigan 48823, USA (Received 18 October 2021; accepted 8 February 2022; published 17 March 2022) We explore the relationship between the quantum state of a compact matter source and of its asymptotic graviton field. For a matter source in an energy eigenstate, the graviton state is determined at leading order by the energy eigenvalue. Insofar as there are no accidental energy degeneracies there is a one to one map between graviton states on the boundary of spacetime and the matter source states. Effective field theory allows us to compute a purely quantum gravitational effect which causes the subleading asymptotic behavior of the graviton state to depend on the internal structure of the source. This establishes the existence of ubiquitous quantum hair due to gravitational effects. \n",
      "------------------------\n",
      "Summary:\n",
      "Reading skills in children with autism spectrum disorder are Het- erogeneous.\n",
      " ----------- Abstract:\n",
      "  This study investigated reading skills in 41 children with autism spectrum disorder. Four components of reading skill were assessed: word recognition, nonword decoding, text reading accuracy and text comprehension. Overall, levels of word and nonword reading and text reading accuracy fell within average range although reading comprehension was impaired. However, there was consid- erable variability across the sample with performance on most tests ranging from ﬂoor to ceiling levels. Some chil- dren read accurately but showed very poor comprehension, consistent with a hyperlexia reading proﬁle; some children were poor at reading words and nonwords whereas others were unable to decode nonwords, despite a reasonable level of word reading skill. These ﬁndings demonstrate the het- erogeneous nature of reading skills in children with ASD. Keywords Reading Æ Language Æ Comprehension Æ Autism Æ Hyperlexia \n",
      "------------------------\n",
      "Summary:\n",
      "The ROUGE project aims to improve the quality of summaries created by computers.\n",
      " ----------- Abstract:\n",
      "   ROUGE stands for Recall-Oriented Understudy for  Gisting Evaluation. It includes measures to auto- matically determine the quality of a summary by  comparing it to other (ideal) summaries created by  humans. The measures count the number of over- lapping units such as n-gram, word sequences, and  word pairs between the computer-generated sum- mary to be evaluated and the ideal summaries cre- ated by humans. This paper introduces four different  ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W,  and ROUGE-S included in the ROUGE summariza- tion evaluation package and their evaluations. Three  of them have been used in the Document Under- standing Conference (DUC) 2004, a large-scale  summarization evaluation sponsored by NIST.  1  \n",
      "------------------------\n",
      "Summary:\n",
      "We present a new package of publicly available re- sources for Spoken Language Understanding.\n",
      " ----------- Abstract:\n",
      "  Spoken Language Understanding infers se- mantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applica- tions. However, publicly available SLU re- sources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is sub- stantially bigger and linguistically more di- verse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed er- ror analysis for identifying potential areas of improvement. SLURP is available at https: //github.com/pswietojanski/slurp 1 \n",
      "------------------------\n",
      "Summary:\n",
      "All papers are copyrighted.\n",
      " ----------- Abstract:\n",
      " University of Wollongong Research Online Faculty of Social Sciences - Papers Faculty of Social Sciences 2017 The importance of critical life moments: An explorative study of successful women with autism spectrum disorder Amanda A. Webster University of Wollongong, awebster@uow.edu.au Susanne Garvis University of Gothenburg Research Online is the open access institutional repository for the University of Wollongong. For further information contact the UOW Library: research-pubs@uow.edu.au Publication Details Webster, A. A. & Garvis, S. (2017). The importance of critical life moments: An explorative study of successful women with autism spectrum disorder. Autism: the international journal of research and practice, 21 (6), 670-677. \n",
      "------------------------\n",
      "Summary:\n",
      "In this paper, we examine the current state-of-the-art NLP models that have been employed for optimal performance and efficiency.\n",
      " ----------- Abstract:\n",
      "  In recent years, Natural Language Processing (NLP) models have achieved phenomenal success in linguistic and semantic tasks like text classiﬁcation, machine translation, cognitive dialogue systems, information retrieval via Natural Language Understanding (NLU), and Natural Language Generation (NLG). This feat is primarily attributed due to the seminal Transformer architecture, leading to designs such as BERT, GPT (I, II, III), etc. Although these large-size models have achieved unprecedented performances, they come at high computational costs. Consequently, some of the recent NLP architectures have utilized concepts of transfer learning, pruning, quantization, and knowledge distillation to achieve moderate model sizes while keeping nearly similar performances as achieved by their predecessors. Additionally, to mitigate the data size challenge raised by language models from a knowledge extraction perspective, Knowledge Retrievers have been built to extricate explicit data documents from a large corpus of databases with greater efﬁciency and accuracy. Recent research has also focused on superior inference by providing efﬁcient attention to longer input sequences. In this paper, we summarize and examine the current state-of-the-art (SOTA) NLP models that have been employed for numerous NLP tasks for optimal performance and efﬁciency. We provide a detailed understanding and functioning of the different architectures, a taxonomy of NLP designs, comparative evaluations, and future directions in NLP. INDEX TERMS Deep learning, natural language processing (NLP), natural language understanding (NLU), natural language generation (NLG), information retrieval (IR), knowledge distillation (KD), pruning, quantization. I. \n",
      "------------------------\n",
      "Summary:\n",
      "The purpose of this study was to systematically review evidence on the camouflage effect in females with autism.\n",
      " ----------- Abstract:\n",
      "  Autism spectrum disorder (ASD) is a neurodevelopmental disorder with increasing prevalence, and a male-to-female ratio  of 4:1. Research has been suggesting that discrepancy in prevalence may be due to the fact that females camouflage their  symptoms. In this study, we aimed to systematically review evidence on the camouflage effect in females with ASD. Follow- ing the PRISMA guidelines, we reviewed empirical research published from January 2009 to September 2019 on PubMed,  Web of Science, PsychInfo and Scopus databases. Thirteen empirical articles were included in this review. Overall, evidence  supports that camouflaging seems to be an adaptive mechanism for females with ASD, despite the negative implications of  these behaviours in their daily life. Keywords Camouflage · Females · Autism · Systematic review Autism spectrum disorder (ASD) is characterized by the  presence of core impairments in neurodevelopmental areas  as communication and social interaction, restricted, repeti- tive and inflexible behaviours, interests and activities and  sensory-perceptual alterations (American Psychiatric Asso- ciation [APA] 2013). ASD has an overall prevalence of 16.8  cases per 1000 (Baio et al. 2018). ASD is diagnosed between  3 to 4 times more in males than in females (Baio et al. 2018),  depending on the used estimates (e.g., active and passive  case ascertainment studies), suggesting a possible diagnostic  gender bias, making girls less prone to be misdiagnosed,  diagnosed at later ages or even missing out a clinical ASD  diagnosis (Loomes et al. 2017). Sex-differences have been observed in relation to the  prevalence of ASD diagnosis in females (Giarelli et al. 2010;  Begeer et al. 2013; Rutherford et al. 2016; Rabbitte et al.  2017; Ratto et al. 2018), which may be associated with dis- tinct autistic traits in females, with some studies referring  the existence of a specific “female phenotype of autism”  (Kopp and Gillberg, 2011; Lai et al. 2015; Bargiela et al.  2016). Some different lines of evidence have been docu- mented that support different clinical presentation of autistic  María Tubío-Fungueiriño and Sara Cruz have contributed equally  to the study. Electronic supplementary material The online version of this  article (https ://doi.org/10.1007/s1080 3-020-04695 -x) contains  supplementary material, which is available to authorized users.  * Montse Fernández-Prieto    montse.fernandez.prieto@usc.es 1  Genomics and Bioinformatics Group, Centre for Research  in Molecular Medicine and Chronic Diseases (CiMUS),  Universidade de Santiago de Compostela (USC), Avda. de  Barcelona, s/n, 15782 Santiago de Compostela, Spain 2  Grupo de Medicina Xenómica, Universidade de Santiago de  Compostela (USC), Santiago de Compostela, Spain 3  Fundación Pública Galega de Medicina Xenómica, Servicio  Galego de Saúde (SERGAS), Santiago de Compostela, Spain 4  Psychology for Positive Development Research Center,  Universidade Lusíada - Norte, Porto, Portugal 5  Instituto de Neurodesenvolvimento (IND), Porto, Portugal 6  Psychological Neuroscience Lab, CIPsi, School  of Psychology, University of Minho, Campus de Gualtar,  4710-057 Braga, Portugal 7  Grupo de Medicina Xenómica, U-711, Centro de  Investigación en Red de Enfermedades Raras (CIBERER),  Universidade de Santiago de Compostela, (USC),  Santiago de Compostela, Spain 8  Grupo de Genética, Instituto de Investigación Sanitaria de  Santiago (IDIS), Santiago de Compostela, Spain \n",
      "------------------------\n",
      "Summary:\n",
      "We have shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks.\n",
      " ----------- Abstract:\n",
      "  This paper shows that pretraining multilingual language models at scale leads to signiﬁcant performance gains for a wide range of cross- lingual transfer tasks. We train a Transformer- based masked language model on one hundred languages, using more than two terabytes of ﬁl- tered CommonCrawl data. Our model, dubbed XLM-R, signiﬁcantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accu- racy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource lan- guages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previ- ous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and ca- pacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the ﬁrst time, the possibility of multilingual modeling without sacriﬁcing per- language performance; XLM-R is very compet- itive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.1 1 \n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(summaries)):\n",
    "    print(\"Summary:\\n\" + summaries[s][0] + \"\\n ----------- Abstract:\\n\", abstracts[s])\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32e34a31e9850d281a94eca83fa3184001b635ea735a6fce5de4560ef755602a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
